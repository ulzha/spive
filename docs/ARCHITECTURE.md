## Architecture

### Spīve Architecture Strategy

Spīve platform is a facade/glue that sits on top of technology stacks for event stream storage and processing (often underpinned by some of: Apache Kafka, Apache Pulsar, Google Cloud Pub/Sub, Amazon Kinesis, Kubernetes), and offers developers a simple abstraction for writing event-sourced applications, as well as a UI for operating/managing these applications.

TODO image

In some sense it comes with "less is more" mindset, exposing only a coherent subset of functionality that is supported in the underlying processing frameworks (`Runner` classes), a subset that is tailored for event-sourced applications. (Notably - ordered persistent immutable streams, reliable at-least-once consumption.) Spīve concepts should constitute a set of building blocks that are easy to reason about, so that application development becomes less bug-prone, and are not too "leaky", so that application owners experience minimal overhead in operation, in terms of needing to understand internals and work around idiosyncrasies.

(Dataflow assumes X minutes maximum processing time per element; Pub/Sub allows maximum Y MB message size - some examples of idiosyncrasies that application developers maybe should not be forced to work around/should have simple ways to work around.)

Compared to other go-to choices (TODO a table someplace else):
* Examples of overhead that we want to avoid is also "slow and bulky" development experience which Kubernetes is notorious for.
* Cloud Pub/Sub says "Click Pull to view messages and temporarily delay message delivery to other subscribers." - we want more user-friendly UIs, would like to simply view messages, and not spend many clicks creating a subscription, and not expose users to awkward delayed delivery concerns)
* Spīve applications may hog resources as they wish (application owners are billed for them). Heterogeneous runners can be made available (e.g. GPU)... Co-location not solved here.
* Many "visual pipeline builders" offer limited and idiosyncratic extension capabilities... Spīve offers an intuitive visual overview of the running applications, while the SDK comprises simple POJIs (Plain Old Java Interfaces) and access to bare VM.

One of the goals is to steer developers focus away from premature optimization, and towards fast iteration on their business logic, and ultimately towards successful/frictionless collaboration between many application developer teams, and integration of their streaming applications into a thriving connected company-wide ecosystem on the platform. Therefore, Spīve prefers not to expose myriads of mode switches and tuning knobs, in favor of investing into end-to-end performance observability and robust rule-based controls to complement it. (Cf. a space launch service quote - as a customer, you provide your mission profile and get launch vehicle suggestions along with prices, as opposed to configuring rocket fuel tank pressures yourself.)

Deliberately, capabilities to tune worker count and similar aspects are left out from application API; rather, control plane services expose higher level APIs, making these augmentable behaviors of the platform. These APIs, in turn, hide the identities of applications being tuned, but rather provide arguments that specify e.g. gateway, storage and processing environment, as well as service level objective and indicator values. There are example rules (TODO link), and SpiveOptimizer exists as an experimentation harness to measure the effects of proposed optimization rules in a live setting, able to speculatively try it out on increasingly larger instances. (See Surrounding Tooling below.) A good rule ought to have a unique applicability domain where it notably improves performance metrics compared to all the other rules known to SpiveOptimizer.

Application developer convenience comes first, and trumps extension developer convenience. In practical terms, this means that contributed Types, Gateways, EventStores must pass uniform, highly demanding quality/versatility milestones. (TODO cf. Urs Hölzle regarding "horizontals".) Read more about the concrete requirements in [API reference].

Another goal with this architecture is that event storages and processing frameworks should be exchangeable transparently, requiring no changes to the developed application code. This is to facilitate platform-wide technology migrations by infrastructure teams, and to maximize benefit from optimization rules invented by individual contributors anywhere.

#### Data Plane

Business logic is packaged in IoC manner within an event loop (code generated by Spīve, yet human-readable for developer convenience) as a deployable artifact, like a JAR or a Docker image. The event loop layer bundles specifics of concrete processing frameworks and storages that the application code doesn't get exposed to. The loop starts by explicitly creating connections to the concrete storages and gateways the instance needs to interact with, then reads each input, invokes the event handlers, and writes each output.

The lifecycle of event loop instances on a particular processing framework is managed by Spīve `Runner`s, which exist as thin agents in the processing frameworks, providing service to the control plane. Example classes could be `InProcessThreadRunner`,`KubernetesRunner`, `DataflowRunner`, `ModalRunner`, `FlinkRunner`, `OpenStackRunner` etc.

`Runner`s are unaware of each other (can be isolated to form security domains). `Runner`s send heartbeat (indication of instance progress in terms of position in its input streams) and error information as a best effort (for KTLO and troubleshooting purposes) over an umbilical interface to the control plane. They stay agnostic of data or schemas being processed. (Probably not partition key ranges and error/warning tracebacks.)

Each event loop consumes a subset of partitions of the event stream, whose events it handles effectively serially. Each event loop is co-located with zero or more running Workloads.

#### Control Plane

At the core of the platform, Spive (`io.ulzha.spive.app.Spive`) is intended to only provide the basic web frontend and API for managing the applications on the platform, as well as the minimum viable KTLO functionality. Without outside intervention it would only ensure that event handling is retried forever in case of failures, and turn stuck instances off and on again, by redeploying them while ensuring correct replay of their input events.

The control plane logic itself exists either as a local process (on a single machine), or as a distributed process (running on _another_, outer Spīve platform), depending on the scale and availability required.

TODO image

The API boundary is designed so that only the common module needs ITGC, while the surrounding tooling does not. Spive API participates in tamper-proofing the logic of applications running on the platform.

A `Runner` availability zone is a resource that must be provided to the control plane, and uniformly locatable within a given Spive deployment. Instance survivability may vary from one `Runner` class to another. Even where instances are disruptable and ephemeral, kinks are ironed out by replay, in such a way as to preserve event-sourcing invariants.

#### Surrounding Tooling (Birds-eye Plane?)

In order for core functionality to remain robust and nimble, additional platform features are often best implemented as separate microservices. Such applications may listen to Spive events and may command it via API calls.

* An essential application of this kind is SpiveScaler. It accumulates performance statistics and makes decisions to divide up stream partitions into an appropriate number of shards, and to assign a suitable number of redundant instances for each shard and workload, to ensure their steady forward progress.[1]

* Creating backups (copying of event streams onto more durable and cheap long term storage media) is another example of an additional feature that a generic tool can manage. SpiveBackup can spawn a dedicated backup creation process for any application that needs its events backed up.

* SpiveOptimizer, SpiveChaos, SomethingGrafanaSomething, SpiveCompactor

[1] The data plane has to be provisioned with appropriate capacity. SpiveScaler's scaling is optimistic, constrained by the fleet size of the agents and the processing capacity that they are servicing. SpiveInventory implements tooling to track availability and usage of this capacity, and alert on costs and quotas. Note that even though cloud agents like `GkeRunner` can have autoscaling in operation for their cluster, there is anyway some ceiling where capacity is effectively manually managed (think increasing quotas on cloud platforms, or expanding bare metal datacenters).

### Scalability

Partition write rate bounded by persistent storage write rate

TODO elaborate
